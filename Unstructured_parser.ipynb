{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"data/\"\n",
    "file_path ='data/Kazakhstan_tarihi_7_atamura_sample.pdf'\n",
    "\n",
    "DISCIPLINE = \"Қазақстан тарихы\"\n",
    "GRADE = \"10\"\n",
    "PUBLISHER = \"Мектеп\"\n",
    "\n",
    "collection_name = \"Docker-Redis_test_sample_tests_3\"\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    # infer_table_structure=True,            # extract tables\n",
    "    strategy=\"ocr_only\",                     # mandatory to infer tables\n",
    "\n",
    "    languages=[\"kaz\"],\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=20000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=5000,       # defaults to 0\n",
    "    new_after_n_chars=10000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")\n",
    "\n",
    "\n",
    "# # Add metadata to chunks\n",
    "for chunk in chunks:\n",
    "    chunk.metadata.discipline = DISCIPLINE\n",
    "    chunk.metadata.grade = GRADE\n",
    "    chunk.metadata.publisher = PUBLISHER\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232288a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([str(type(el)) for el in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7066be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image as PILImage  # Rename to avoid conflict with IPython.display.Image\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def display_base64_image(b64_code):\n",
    "    try:\n",
    "        image_data = base64.b64decode(b64_code)\n",
    "\n",
    "        # 3. Filter: Check if width is greater than limit\n",
    "        display(Image(data=image_data))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def display_images(base64_list):\n",
    "    \n",
    "    for i, b64_code in enumerate(base64_list):\n",
    "        try:\n",
    "            display_base64_image(b64_code)     \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "    if \"CompositeElement\" in str(type(chunk)):\n",
    "        texts.append(chunk)\n",
    "\n",
    "tables, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f990b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_context(chunks):\n",
    "    images_context = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for i,el in enumerate(chunk_els):\n",
    "                \n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_context.append({\"context_text\":chunk.text,\"image_base64\": el.metadata.image_base64})\n",
    "\n",
    "    return images_context\n",
    "\n",
    "images_context = get_images_context(chunks)\n",
    "\n",
    "# 2. Create the second list by extracting references from the first list\n",
    "# Do NOT call get_images_base64(chunks) here.\n",
    "images = [item[\"image_base64\"] for item in images_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa40d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0dc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- VERIFICATION ---\n",
    "addr_1 = id(images_context[0][\"image_base64\"])\n",
    "addr_2 = id(images[0])\n",
    "\n",
    "print(f\"Address in Dict: {addr_1}\")\n",
    "print(f\"Address in List: {addr_2}\")\n",
    "print(f\"Are they the same object? {addr_1 == addr_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba55b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_images_context(text_image_list, min_width=150, min_height=150, max_width=1500, max_height=1500 ):\n",
    "    print(f\"Filtering for images wider than {min_width}px...\\n\")\n",
    "\n",
    "    filtered_images = []\n",
    "    \n",
    "    for i, text_image in enumerate(text_image_list):\n",
    "        try:\n",
    "            # 1. Decode base64 to bytes\n",
    "            image_data = base64.b64decode(text_image[\"image_base64\"])\n",
    "            \n",
    "            # 2. Read image metadata using Pillow (without saving to disk)\n",
    "            with PILImage.open(io.BytesIO(image_data)) as img:\n",
    "                width, height = img.size\n",
    "            \n",
    "            # 3. Filter: Check if width is greater than limit\n",
    "            if width > min_width and width < max_width and height > min_height and height < max_height:\n",
    "                print(f\"✅ Image {i}: {width}x{height} px\")\n",
    "                # display(Image(data=image_data))\n",
    "                filtered_images.append(text_image)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Skipped Image {i}: {width}x{height} px \")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {e}\")\n",
    "    return filtered_images\n",
    "\n",
    "# Usage\n",
    "# 6th atamura\n",
    "# filtered_images_context = filter_images_context(images_context, max_height=10000, max_width=10000)\n",
    "# filtered_images = [item[\"image_base64\"] for item in filtered_images_context]\n",
    "# 7th atamura\n",
    "filtered_images_context = filter_images_context(images_context, max_height=1600, max_width=1200)\n",
    "filtered_images = [item[\"image_base64\"] for item in filtered_images_context]\n",
    "# filtered_images = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VERIFICATION ---\n",
    "if filtered_images_context:\n",
    "    addr_1 = id(filtered_images_context[0][\"image_base64\"])\n",
    "    addr_2 = id(images[0])\n",
    "\n",
    "    print(f\"Address in Dict: {addr_1}\")\n",
    "    print(f\"Address in List: {addr_2}\")\n",
    "    print(f\"Are they the same object? {addr_1 == addr_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(filtered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1932ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50805f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text in kazakh language.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-3-flash-preview\", api_key=GEMINI_API_KEY, temperature=1)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc98ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize text\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 10})\n",
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e828f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summarize tables\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 10})\n",
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Describe the image in detail in kazakh language.\n",
    "                This is a context text where the image appears:\n",
    "                {context_text}\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_base64}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "\n",
    "image_summaries = chain.batch(filtered_images_context, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from qdrant_client import QdrantClient, models\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain_classic.retrievers import MultiVectorRetriever\n",
    "\n",
    "\n",
    "QDRANT_API = os.getenv(\"QDRANT_API\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd084f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_classic.storage import EncoderBackedStore\n",
    "from langchain_classic.retrievers import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "\n",
    "# --- 1. Qdrant Setup (Your existing code) ---\n",
    "client = QdrantClient(location=QDRANT_URL, api_key=QDRANT_API)\n",
    "\n",
    "\n",
    "# Check if collection exists to avoid errors on restart\n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=768, \n",
    "            distance=models.Distance.COSINE\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Note the \"metadata.\" prefix\n",
    "nested_fields = [\"metadata.discipline\", \"metadata.publisher\", \"metadata.grade\"]\n",
    "\n",
    "for field in nested_fields:\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=field,\n",
    "        field_schema=models.PayloadSchemaType.KEYWORD\n",
    "    )\n",
    "    print(f\"Index created for nested field: '{field}'\")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/gemini-embedding-001\", \n",
    "        api_key=GEMINI_API_KEY,\n",
    "        output_dimensionality=768\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- 2. Redis Storage Setup (Replaces LocalFileStore) ---\n",
    "\n",
    "# A. Create the Base Store (Handles raw bytes in Redis)\n",
    "# 'namespace' adds a prefix to keys (e.g. \"parent_docs:doc_id\") so they are organized\n",
    "\n",
    "\n",
    "redis_byte_store = RedisStore(\n",
    "    # redis_url=REDIS_URL,\n",
    "    redis_url=\"redis://localhost:6379\", \n",
    "    namespace=\"parent_docs\"\n",
    ")\n",
    "\n",
    "# B. Define Serializers (Object -> JSON Bytes)\n",
    "# JSON is safer and cleaner than pickle for production\n",
    "def json_encoder(obj: Document) -> bytes:\n",
    "    if hasattr(obj, \"to_dict\"):\n",
    "        return json.dumps(obj.to_dict())\n",
    "    # If it's already a string (like your base64 images), just dump it\n",
    "    return json.dumps(obj)\n",
    "\n",
    "\n",
    "def json_decoder(data):\n",
    "    \"\"\"Восстанавливаем объект из JSON-строки\"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    dict_data = json.loads(data)\n",
    "    \n",
    "    # Проверяем, является ли это словарем от unstructured (наличие типа элемента)\n",
    "    if isinstance(dict_data, dict) and \"type\" in dict_data:\n",
    "        # dict_to_elements ожидает список, поэтому оборачиваем в []\n",
    "        elements = dict_to_elements([dict_data])\n",
    "        return elements[0]\n",
    "    \n",
    "    return dict_data\n",
    "\n",
    "# C. Create the \"Smart\" Store\n",
    "# This wraps Redis to automatically handle Document objects\n",
    "store = EncoderBackedStore(\n",
    "    store=redis_byte_store,\n",
    "    key_encoder=lambda x: x, \n",
    "    value_serializer=json_encoder,\n",
    "    value_deserializer=json_decoder\n",
    ")\n",
    "\n",
    "# --- 3. The Retriever ---\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store, \n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab16467",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "summary_texts = []\n",
    "valid_texts = [] # List to store only texts that have valid summaries\n",
    "\n",
    "for i, summary in enumerate(text_summaries):\n",
    "    # Check if summary exists and is not whitespace\n",
    "    if summary and summary.strip():\n",
    "        current_id = str(uuid.uuid4())\n",
    "        doc_ids.append(current_id)\n",
    "        \n",
    "        # Capture the corresponding raw text\n",
    "        valid_texts.append(texts[i])\n",
    "        \n",
    "        summary_texts.append(\n",
    "            Document(\n",
    "                page_content=summary, \n",
    "                metadata={\n",
    "                    id_key: current_id,\n",
    "                    # ADD YOUR METADATA HERE\n",
    "                    \"discipline\": DISCIPLINE,\n",
    "                    \"publisher\": PUBLISHER,\n",
    "                    \"grade\": GRADE\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Text summary {i} is empty. Skipping.\")\n",
    "\n",
    "# Add to vectorstore and docstore only if valid data exists\n",
    "if summary_texts:\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, valid_texts)))\n",
    "    print(f\"Successfully inserted {len(summary_texts)} text documents.\")\n",
    "else:\n",
    "    print(\"No valid text summaries found to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaae7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ids = []\n",
    "summary_tables = []\n",
    "valid_tables = [] # List to store only tables that have valid summaries\n",
    "\n",
    "for i, summary in enumerate(table_summaries):\n",
    "    # Check if summary exists and is not whitespace\n",
    "    if summary and summary.strip():\n",
    "        current_id = str(uuid.uuid4())\n",
    "        table_ids.append(current_id)\n",
    "        \n",
    "        # Capture the corresponding raw table\n",
    "        valid_tables.append(tables[i])\n",
    "        \n",
    "        summary_tables.append(\n",
    "            Document(\n",
    "                page_content=summary,              \n",
    "                metadata={\n",
    "                    id_key: current_id, \n",
    "                    \"discipline\": DISCIPLINE,\n",
    "                    \"publisher\": PUBLISHER,\n",
    "                    \"grade\": GRADE\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Table summary {i} is empty. Skipping.\")\n",
    "\n",
    "# Add to vectorstore and docstore only if valid data exists\n",
    "if summary_tables:\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, valid_tables)))\n",
    "    print(f\"Successfully inserted {len(summary_tables)} tables.\")\n",
    "else:\n",
    "    print(\"No valid table summaries found to insert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c63e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Создаем список ID и документов, но добавляем проверку на пустоту\n",
    "img_ids = []\n",
    "summary_img = []\n",
    "\n",
    "for i, summary in enumerate(image_summaries):\n",
    "    # Проверяем, что summary не пустое и не состоит только из пробелов\n",
    "    if summary and summary.strip():\n",
    "        current_id = str(uuid.uuid4())\n",
    "        img_ids.append(current_id)\n",
    "        summary_img.append(\n",
    "            Document(\n",
    "                page_content=summary,             \n",
    "                metadata={\n",
    "                    id_key: current_id, \n",
    "                    \"discipline\": DISCIPLINE,\n",
    "                    \"publisher\": PUBLISHER,\n",
    "                    \"grade\": GRADE\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Image summary {i} is empty. Skipping this image.\")\n",
    "\n",
    "# 2. Соответственно, фильтруем исходные base64 изображения, чтобы они совпадали с summary_img по индексам\n",
    "# (docstore должен получить только те изображения, для которых есть описания)\n",
    "valid_filtered_images = [img for i, img in enumerate(filtered_images) if image_summaries[i] and image_summaries[i].strip()]\n",
    "\n",
    "# 3. Добавляем только валидные документы\n",
    "if summary_img:\n",
    "    retriever.vectorstore.add_documents(summary_img)\n",
    "    retriever.docstore.mset(list(zip(img_ids, valid_filtered_images)))\n",
    "    print(f\"Successfully inserted {len(summary_img)} images.\")\n",
    "else:\n",
    "    print(\"No valid image summaries found to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ONLY within images for \"user_123\"\n",
    "query = \"жонгар\"\n",
    "\n",
    "search_filter = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"metadata.grade\", # <--- Changed from \"grade\" to \"metadata.grade\"\n",
    "            match=models.MatchValue(value=\"10\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the filter\n",
    "retriever.search_kwargs.update({\"filter\": search_filter})\n",
    "retriever.search_kwargs.update({\"k\":5})\n",
    "\n",
    "# Run query\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_images(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9a5c5",
   "metadata": {},
   "source": [
    "# Render PDF page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e257b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image as PILImage  # Rename to avoid conflict with IPython.display.Image\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# 1. PLOTTING FUNCTION\n",
    "def plot_pdf_with_boxes(pdf_page, segments):\n",
    "    pix = pdf_page.get_pixmap()\n",
    "    pil_image = PILImage.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "\n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(pil_image)\n",
    "    \n",
    "    categories = set()\n",
    "    category_to_color = {\n",
    "        \"Title\": \"orchid\",\n",
    "        \"Image\": \"forestgreen\",\n",
    "        \"Table\": \"tomato\",\n",
    "    }\n",
    "    \n",
    "    for segment in segments:\n",
    "        points = segment[\"coordinates\"][\"points\"]\n",
    "        layout_width = segment[\"coordinates\"][\"layout_width\"]\n",
    "        layout_height = segment[\"coordinates\"][\"layout_height\"]\n",
    "        \n",
    "        # Scale points to match image dimensions\n",
    "        scaled_points = [\n",
    "            (x * pix.width / layout_width, y * pix.height / layout_height)\n",
    "            for x, y in points\n",
    "        ]\n",
    "        \n",
    "        # Use .get() for the dictionary color lookup, defaulting to deepskyblue\n",
    "        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")\n",
    "        categories.add(segment[\"category\"])\n",
    "        \n",
    "        rect = patches.Polygon(\n",
    "            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Make legend\n",
    "    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]\n",
    "    for category in [\"Title\", \"Image\", \"Table\"]:\n",
    "        if category in categories:\n",
    "            legend_handles.append(\n",
    "                patches.Patch(color=category_to_color[category], label=category)\n",
    "            )\n",
    "    ax.axis(\"off\")\n",
    "    ax.legend(handles=legend_handles, loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # FIX: Explicitly close the figure to prevent \"<Figure size...>\" logs\n",
    "    plt.close(fig) \n",
    "\n",
    "# 2. RENDER PAGE FUNCTION\n",
    "def render_page(doc_list: list, page_number: int, print_text=True) -> None:\n",
    "    # Ensure 'file_path' is defined in your global scope or passed in\n",
    "    pdf_page = fitz.open(file_path).load_page(page_number - 1)\n",
    "    \n",
    "    page_docs = [\n",
    "        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number\n",
    "    ]\n",
    "    segments = [doc.metadata for doc in page_docs]\n",
    "    \n",
    "    plot_pdf_with_boxes(pdf_page, segments)\n",
    "    \n",
    "    if print_text:\n",
    "        for doc in page_docs:\n",
    "            print(f\"{doc.page_content}\\n\")\n",
    "\n",
    "# 3. HELPER FUNCTION\n",
    "def extract_page_numbers_from_chunk(chunk):\n",
    "    # REVERTED: Back to dot notation as requested\n",
    "    elements = chunk.metadata.orig_elements\n",
    "    page_numbers = set()\n",
    "    for element in elements:\n",
    "        # Check if 'element.metadata' is an object or dict. \n",
    "        # Using dot notation based on your previous working code.\n",
    "        if element.metadata.page_number:\n",
    "            page_numbers.add(element.metadata.page_number)\n",
    "\n",
    "    return page_numbers\n",
    "\n",
    "# 4. MAIN DISPLAY FUNCTION\n",
    "def display_chunk_pages(chunk):\n",
    "    page_numbers = extract_page_numbers_from_chunk(chunk)\n",
    "    docs = []\n",
    "    # REVERTED: Back to dot notation\n",
    "    for element in chunk.metadata.orig_elements:\n",
    "        metadata = element.metadata.to_dict()\n",
    "\n",
    "        \n",
    "        # Determine Category based on element type\n",
    "        if \"Table\" in str(type(element)):\n",
    "            metadata[\"category\"] = \"Table\"\n",
    "        elif \"Image\" in str(type(element)):\n",
    "            metadata[\"category\"] = \"Image\"\n",
    "        else:\n",
    "            metadata[\"category\"] = \"Text\"\n",
    "            \n",
    "        metadata[\"page_number\"] = int(element.metadata.page_number)\n",
    "        \n",
    "        docs.append(Document(page_content=element.text, metadata=metadata))\n",
    "\n",
    "    # Render every page found in this chunk\n",
    "    for page_number in page_numbers:\n",
    "        render_page(docs, page_number)\n",
    "\n",
    "# usage\n",
    "# display_chunk_pages(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.metadata.page_number)\n",
    "    print(\"-\"*100)\n",
    "    display_chunk_pages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77554b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from base64 import b64decode\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"Разделяем изображения (строки base64) и тексты (объекты unstructured)\"\"\"\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        # Картинки в Redis мы сохраняли как обычные строки (json.dumps строки)\n",
    "        if isinstance(doc, str):\n",
    "            b64.append(doc)\n",
    "        else:\n",
    "            # Все остальное (тексты и таблицы) — это объекты классов unstructured\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += f\"The Discipline: {text_element.metadata.discipline} \\n\"\n",
    "            context_text += f\"The Grade: {text_element.metadata.grade} \\n\"\n",
    "            context_text += f\"The Publisher: {text_element.metadata.publisher} \\n\"\n",
    "            context_text += f\"The Page number: {text_element.metadata.page_number} \\n\\n\"\n",
    "            context_text += text_element.text + \"\\n\\n\"\n",
    "\n",
    "    \n",
    "    system_instruction = \"\"\"\n",
    "    You are an expert UNT (Unified National Testing) tutor in Kazakhstan, specializing in preparing students for high-stakes exams.\n",
    "    Your goal is not just to answer, but to help the student understand the material based strictly on the provided text.\n",
    "\n",
    "    ### STRICT DATA BOUNDARIES\n",
    "    - Answer **ONLY** based on the provided Context.\n",
    "    - If the answer is not in the context, explicitly state: \"Мәтінде бұл сұрақтың жауабы жоқ\" (if Kazakh) or \"В тексте нет ответа на этот вопрос\" (if Russian). Do not make up information.\n",
    "\n",
    "    ### RESPONSE FORMAT\n",
    "    1. **Direct Answer**: Start with a clear, direct answer to the question.\n",
    "    2. **Explanation**: Provide a long sentence explanation citing the context (e.g., \"Because the text mentions...\").\n",
    "    3. **Questions**: Ask 2-3 questions from the context to ensure that students understand the material\n",
    "    4. **Source**: Necessarily provide information sources that you have used (Discipline, Grade, Publisher, and Page number)\n",
    "\n",
    "    ### TONE & STYLE\n",
    "    - **Language**: Strictly mirror the user's language (Kazakh or Russian).\n",
    "    - **Format**: Use bullet points for readability.\n",
    "    \"\"\"\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=system_instruction),\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58177c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\", api_key=GEMINI_API_KEY, temperature=1)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"кого ты видишь на картинке?\"\n",
    "\n",
    "response = chain_with_sources.invoke(\n",
    "    query\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.text) \n",
    "    # Замените text[\"metadata\"][\"page_number\"] на text.metadata.page_number\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_chain(question: str):\n",
    "    # 1. Retrieve Documents\n",
    "    # We explicitly call .invoke() on the retriever\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # 2. Parse Documents (Split into Text and Images)\n",
    "    # We call your custom function directly\n",
    "    parsed_context = parse_docs(retrieved_docs)\n",
    "    \n",
    "    # 3. Build the Prompt\n",
    "    # We manually create the dictionary that build_prompt expects\n",
    "    prompt_arguments = {\n",
    "        \"context\": parsed_context,\n",
    "        \"question\": question\n",
    "    }\n",
    "    messages = build_prompt(prompt_arguments)\n",
    "    \n",
    "    # 4. Generate Response (Run the Model)\n",
    "    # passing the list of messages directly to the LLM\n",
    "    ai_message = model.invoke(messages)\n",
    "    \n",
    "    # 5. Parse Output (Get the string content)\n",
    "    response_string = ai_message.content\n",
    "    \n",
    "    # 6. Return Final Result\n",
    "    # This matches exactly what the 'assign' chain would have returned\n",
    "    return {\n",
    "        \"context\": parsed_context,\n",
    "        \"question\": question,\n",
    "        \"response\": response_string\n",
    "    }\n",
    "\n",
    "# --- How to use it ---\n",
    "result = run_rag_chain(\"Бұмын не істеді?\")\n",
    "\n",
    "print(\"--- ANSWER ---\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "print(\"\\n--- SOURCES USED ---\")\n",
    "print(result[\"context\"][\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Что ты видишь на картинках?\"\n",
    "\n",
    "# 1. Retrieve Documents\n",
    "# We explicitly call .invoke() on the retriever\n",
    "# retriever.search_kwargs.update({\"k\": 6})\n",
    "# retrieved_docs = retriever.invoke(question)\n",
    "retrieved_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029253fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Parse Documents (Split into Text and Images)\n",
    "# We call your custom function directly\n",
    "parsed_context = parse_docs(retrieved_docs)\n",
    "parsed_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the Prompt\n",
    "# We manually create the dictionary that build_prompt expects\n",
    "prompt_arguments = {\n",
    "    \"context\": parsed_context,\n",
    "    \"question\": question\n",
    "}\n",
    "prompt_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c441b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = build_prompt(prompt_arguments)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create the chain\n",
    "chain = messages | model\n",
    "\n",
    "# 2. Invoke the chain (passing the variables your template needs)\n",
    "# If your template has no variables, pass an empty dict {}\n",
    "ai_message = chain.invoke({\"question\": \"What is in the document?\"})\n",
    "\n",
    "response_string = ai_message.content\n",
    "\n",
    "# 5. Parse Output (Get the string content)\n",
    "response_string = ai_message.content\n",
    "\n",
    "# 6. Return Final Result\n",
    "# This matches exactly what the 'assign' chain would have returned\n",
    "result={\n",
    "    \"context\": parsed_context,\n",
    "    \"question\": question,\n",
    "    \"response\": response_string\n",
    "}\n",
    "response_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72f5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-test (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
